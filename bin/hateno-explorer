#!/usr/bin/env python3
# -*- coding: utf-8 -*-

'''
This script uses the Explorer class to find particular simulations.
'''

import sys
import importlib.util
import argparse
import json
import tabulate

from hateno import jsonfiles
from hateno.explorer import Explorer, ExplorerUI, EvaluationMode

def addArguments(parser):
	parser.add_argument('--config', type = str, required = True, help = 'name of the config to use')
	parser.add_argument('--default', type = argparse.FileType('r'), help = 'path to the file where the default settings are listed')
	parser.add_argument('--map', type = argparse.FileType('r'), help = 'path to the file where the map is described')
	parser.add_argument('--mode', choices = ['each', 'group'], default = 'each', help = 'evaluation mode')
	parser.add_argument('--output', type = str, help = 'path to the file where the output should be written')
	parser.add_argument('--setting', type = str, help = 'setting to test')
	parser.add_argument('--test-values', type = float, nargs = '*', help = 'values to test')
	parser.add_argument('--add-to-manager', action = 'store_true', help = 'add the generated simulations to the manager')
	parser.add_argument('folder_path', type = str, help = 'path to the simulations folder')
	parser.add_argument('evaluation_script', type = argparse.FileType('r'), help = 'path to the script where the evaluation function is defined')

def action(args):
	with Explorer(args.folder_path, args.config, generate_only = not(args.add_to_manager)) as explorer:
		ui = ExplorerUI(explorer)

		if not(args.default is None):
			explorer.default_simulation = jsonfiles.read(args.default.name, allow_generator = True)

		module_name = 'tmpsimulationevaluation'
		spec = importlib.util.spec_from_file_location(module_name, args.evaluation_script.name)
		module = importlib.util.module_from_spec(spec)
		sys.modules[module_name] = module
		spec.loader.exec_module(module)

		if not(args.map is None):
			output = explorer.useMap(jsonfiles.read(args.map.name, allow_generator = True), module.evaluate, evaluation_mode = EvaluationMode[args.mode.upper()])

			if args.output is None:
				print(json.dumps(output, indent = '\t'))

			else:
				jsonfiles.write(output, args.output, sort_keys = False)

		else:
			setting_name = args.setting.split('/')
			map_description = {
				'setting': {
					'set': setting_name[0],
					'name': setting_name[1]
				},
				'values': args.test_values
			}

			output = explorer.useMap(map_description, module.evaluate)

			if args.output is None:
				print(tabulate.tabulate([[o['settings'][0]['value'], o['evaluation']] for o in output['evaluations']], headers = ['Tested value', 'Evaluation'], tablefmt = 'github'))

			else:
				jsonfiles.write(output, args.output, sort_keys = False)

if __name__ == '__main__':
	parser = argparse.ArgumentParser(description = 'Find particular simulations.')
	addArguments(parser)
	action(parser.parse_args())
